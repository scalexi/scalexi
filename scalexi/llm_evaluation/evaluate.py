import pandas as pd
import random
import json
import os
import openai
from openai import OpenAI
import time 
import logging
import httpx
from datetime import datetime
#
# Read logging level from environment variable
logging_level = os.getenv('LOGGING_LEVEL', 'WARNING').upper()

# Configure logging with the level from the environment variable
logging.basicConfig(
    level=getattr(logging, logging_level, logging.WARNING),  # Default to WARNING if invalid level
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Create a logger object
logger = logging.getLogger(__name__)

class LLMEvaluation ():

    def __init__(self, model_name="gpt-3.5-turbo", openai_api_key=None, generator_system_prompt = None, eval_system_prompt=None, enable_timeouts= False, timeouts_options= None):
        self.model_name = model_name
        self.openai_api_key = openai_api_key
        self.client = OpenAI(api_key=self.openai_api_key, max_retries=3)
        if enable_timeouts:
            if timeouts_options is None:
                timeouts_options = {"total": 120, "read": 60.0, "write": 60.0, "connect": 10.0}
                self.client = self.client.with_options(timeout=httpx.Timeout(120.0, read=60.0, write=60.0, connect=10.0))
            else:
                self.client = self.client.with_options(timeout=httpx.Timeout(timeouts_options["total"], timeouts_options["read"], timeouts_options["write"], timeouts_options["connect"]))
        if eval_system_prompt is None:
            self.eval_system_prompt = """Your task is to compare responses generated by a model to correct responses. 
                These responses are compared to a correct response, and a single number (0.0-5.0) is generated for each model.
                Please rate the generated response on a scale of 0.0 to 5.5, with 5.0 being perfectly correct and 0.0 being completely off. 
                The output should be a JSON with score based on the prompt,            
                Also justify your scoring in 12 words. The score_reason should not exceed 12 words and does not include any special characters. "
                "The output must be in VALID JSONL format in one line."
                score_reason should not exceed 12 words and does not include any special characters
                Eg: {
                \"score\": 4.25,
                \"score_reason\": \"12 words reasons without special chars\"
                }"""
        else:
            self.eval_system_prompt = eval_system_prompt

        if generator_system_prompt is None:
            self.generator_system_prompt = """You are an intelligent assistant capable of generating responses based on prompts."""
        else:
            self.generator_system_prompt = generator_system_prompt

    def save_dict_list_to_csv(self, data, output_file_path=None, output_folder='csv'):
        """
        Converts a list of conversation data into a CSV file with system_prompt, prompt, and completion.

        Each conversation is expected to have a list of dictionnary of messages with roles (system, user, assistant) and their content. 
        The CSV file will have columns for system_prompt, user prompt, and assistant completion.

        Parameters
        ----------
        data : list
            A list of dictionaries, where each dictionary represents a conversation and contains a list of message dictionaries.
        output_file : str, optional
            The name of the output CSV file. Default is 'conversation_data.csv'.

        Returns
        -------
        None

        Raises
        ------
        Exception
            If an error occurs during the processing or file writing.

        Example
        -------
        >>> data = [{'messages': [{'role': 'system', 'content': 'System message'}, {'role': 'user', 'content': 'User question'}, {'role': 'assistant', 'content': 'Assistant answer'}]}]
        >>> convert_to_csv(data, 'output.csv')
        """
        # Check if output folder exists, create if not
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        try:
            # Process data to extract prompts and completions
            processed_data = []
            for conversation in data:
                system_prompt = ""
                user_prompt = ""
                assistant_completion = ""

                for message in conversation['messages']:
                    if message['role'] == 'system':
                        system_prompt = message['content']
                    elif message['role'] == 'user':
                        user_prompt = message['content']
                    elif message['role'] == 'assistant':
                        assistant_completion = message['content']

                processed_data.append({
                    'system_prompt': system_prompt,
                    'prompt': user_prompt,
                    'completion': assistant_completion
                })


            # Create DataFrame and Save to CSV
            df = pd.DataFrame(processed_data)
            df.to_csv(output_file_path, index=False)

        except Exception as e:
            raise Exception(f"An error occurred during CSV conversion: {e}")


    def read_jsonl(self, file_path):
        """
        Read a JSONL (JSON Lines) file and return data as a list of dictionaries.
        """
        with open(file_path, 'r') as file:
            return [json.loads(line) for line in file]

    def save_random_prompts(self, input_file, output_file = None, output_format='csv', n_samples=100, output_folder='output'):
        """
        Selects random prompts from a given file and saves them in the specified format.

        Parameters
        ----------
        input_file : str
            The path to the input file (CSV, JSON, or JSONL).
        output_file : str
            The base name of the output file without extension. If None, an automatic name with a timestamp and the number of samples will be appended to this base name.
        output_format : str, optional
            The format to save the output file in. Supported formats are 'csv' for CSV files, 'json' for JSON files, and 'jsonl' for JSON Lines files. The default format is 'csv'.
        n_samples : int, optional
            The number of random samples to select. Default is 100.
        output_folder : str, optional
            The folder where the output file should be saved. Default is 'output'.

        Returns
        -------
        None
        """
        try:
            # Check if output folder exists, create if not
            if not os.path.exists(output_folder):
                os.makedirs(output_folder)

            # Read data
            if input_file.endswith('.csv'):
                data = pd.read_csv(input_file)
                data_dict = data.to_dict(orient='records')
            elif input_file.endswith('.json'):
                data_dict = pd.read_json(input_file, orient='records').to_dict(orient='records')
            elif input_file.endswith('.jsonl'):
                data_dict = self.read_jsonl(input_file)
            else:
                raise ValueError("Unsupported file format. Please provide a CSV, JSON, or JSONL file.")

            # Select random samples
            random_samples = random.sample(data_dict, min(n_samples, len(data_dict)))

            if output_file == None:
                output_file = "random_prompts"
                # Construct output file path with timestamp
                current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
                output_file_name = f"{output_file}_{current_time}_{n_samples}.{output_format}"
                output_file_path = os.path.join(output_folder, output_file_name)
            else:
                output_file, _ = os.path.splitext(output_file)
                # Construct output file path with timestamp
                current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
                output_file_name = f"{output_file}_{current_time}_{n_samples}.{output_format}"
                output_file_path = os.path.join(output_folder, output_file_name)

            # Save data in the desired format
            if output_format == 'csv':
                if input_file.endswith('.jsonl') or input_file.endswith('.json'):
                    self.save_dict_list_to_csv(random_samples, output_file_path)
                else:
                    df = pd.DataFrame(random_samples)  # This creates a DataFrame with columns based on the keys in the dictionaries
                    df.to_csv(output_file_path, index=False)  # Saves the DataFrame to a CSV file without the index column
            elif output_format == 'json':
                with open(output_file_path, 'w') as file:
                    json.dump(random_samples, file, indent=4)
            elif output_format == 'jsonl':
                with open(output_file_path, 'w') as file:
                    for item in random_samples:
                        file.write(json.dumps(item) + '\n')
            else:
                raise ValueError("[save_random_prompts] Unsupported output format. Please choose 'csv', 'json', or 'jsonl'.")
        except Exception as e:
            raise Exception (f"[save_random_prompts] Error: {e}")
        


    def rephrase_and_optionally_classify(self, prompt, 
                                         model_name="gpt-4", 
                                        classify=False, classes=None,
                                        prompt_style='student-asking', 
                                        temperature=1, max_tokens=256, 
                                        top_p=1, 
                                        frequency_penalty=0, 
                                        presence_penalty=0):

        """
        Rephrases a given prompt and optionally classifies it using a specified language model.

        Parameters
        ----------
        prompt : str
            The original sentence that needs to be rephrased.
        model_name : str, optional
            The name of the language model to use. Default is 'gpt-4'.
        classify : bool, optional
            Whether to classify the rephrased sentence. Default is False.
        classes : list of str, optional
            The list of classification categories to be used if classify is True. Default is None.
        prompt_style : str, optional
            The style in which to rephrase the prompt. Used only if classify is False. Default is 'student-asking'.
        temperature : float, optional
            Controls the randomness of the output. Default is 1.
        max_tokens : int, optional
            The maximum number of tokens to generate. Default is 256.
        top_p : float, optional
            Nucleus sampling parameter controlling the range of token probabilities. Default is 1.
        frequency_penalty : float, optional
            Adjusts frequency of token usage. Default is 0.
        presence_penalty : float, optional
            Adjusts presence of tokens. Default is 0.

        Returns
        -------
        tuple
            A tuple containing the rephrased prompt and its classification (or None if not classified).

        Raises
        ------
        Exception
            If an error occurs during the API request or processing.

        Examples
        --------
        >>> rephrased = rephrase_and_optionally_classify("What is AI?", classify=True, classes=["ACADEMIC", "RESEARCH", "ADMIN", "SCIENCE", "OTHERS"])
        >>> print(rephrased)
        """
        try:
            
            # Default classes if not provided
            if classes is None:
                classes = ["ACADEMIC", "RESEARCH", "ADMIN", "SCIENCE", "OTHERS", "BUSINESS", "TECHNOLOGY", "HEALTH", "ENTERTAINMENT", "SPORTS"]
            
            classes_str = ", ".join(classes)

            if classify:
                system_message = f"You are a helpful assistant to rephrase and classify sentences as {classes_str}."
                user_message = f"Please rephrase: '{prompt}' and classify it into one of the categories: {classes_str}. Output format: '{{\"rephrased_prompt\" : \"value\", \"classification\" : \"value\"}}'"
            else:
                system_message = f"You are a helpful assistant that helps rephrase sentences in a '{prompt_style}' style."
                user_message = f"Please rephrase the following sentence: '{prompt}'. Output format: '{{\"rephrased_prompt\" : \"value\"}}'"


            response = self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stop=["\n"]
                )

            logger.debug(response.choices[0].message.content, "\n")
            model_response = response.choices[0].message.content

            if classify:
                response_data = json.loads(model_response)
                return response_data["rephrased_prompt"], response_data["classification"]
            else:
                logger.debug(model_response)
                response_data = json.loads(model_response)
                logger.debug(response_data)
                return response_data["rephrased_prompt"], None

        except json.JSONDecodeError:
            raise ValueError("Failed to parse model response as JSON.")
        except Exception as e:
            raise Exception(f"An unexpected error occurred: {e}")


    def process_and_classify_prompts(self, input_csv, output_csv, model_name="gpt-3.5-turbo", classify=False, classes=None):
        """
        Processes and classifies prompts from an input CSV file and saves the results to an output CSV file.

        Parameters
        ----------
        input_csv : str
            The path to the input CSV file containing prompts and their corresponding completions.
        output_csv : str
            The path to the output CSV file where the processed data with rephrased prompts and classifications will be saved.
        model_name : str, optional
            The name of the language model to use. Default is 'gpt-4'.
        classify : bool, optional
            Whether to classify the rephrased sentence. Default is False.
        classes : list of str, optional
            The list of classification categories to be used if classify is True. Default is None.

        Returns
        -------
        None

        Raises
        ------
        FileNotFoundError
            If the input CSV file is not found.
        Exception
            For other exceptions that may occur during processing.
        """
        # Create the output folder if it doesn't exist
        output_folder = os.path.dirname(output_csv)
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)

        # Load input data
        try:
            df_input = pd.read_csv(input_csv)
        except FileNotFoundError as e:
            logger.error(f"The file {input_csv} was not found: {e}")
            return
        except Exception as e:
            logger.error(f"Error reading input CSV file: {e}")
            return

        # Creating a DataFrame for the output
        df_output = pd.DataFrame(columns=['initial_prompt', 'prompt', 'ground_truth_completion', 'classification'])

        for index, row in df_input.iterrows():
            try:
                initial_prompt = row['prompt']
                completion = row['completion']

                # Use the rephrase_and_optionally_classify method
                rephrased_prompt, classification = self.rephrase_and_optionally_classify(initial_prompt, model_name=model_name, classify=classify, classes=classes)
                #logger.debug(f"rephrased_prompt: {rephrased_prompt}")
                #logger.debug(f"classification: {classification}")
                new_row = {
                    "initial_prompt": initial_prompt,
                    "prompt": rephrased_prompt,
                    "ground_truth_completion": completion,
                    "classification": str(classification)
                }
                # Using pd.concat instead of append
                df_output = pd.concat([df_output, pd.DataFrame([new_row])], ignore_index=True)

                # To avoid hitting rate limits
                time.sleep(0.5)

            except Exception as e:
                logger.error(f"Error processing row {index}: {e}")

        # Save to CSV
        try:
            df_output.to_csv(output_csv, index=False)
        except Exception as e:
            logger.error(f"Error writing to output CSV file: {e}")

    def get_generated_completion(self, 
                                 finetuned_model, 
                                 prompt, 
                                 temperature=1.0, 
                                 max_tokens=256, 
                                 top_p=1.0, 
                                 finetuned_model_start_sequence_only_for_base_models=""):
        """
        Get the generated completion based on the model type.

        Args:
            finetuned_model (str): Name of the model.
            prompt (str): Input prompt for the model.
            temperature (float): Sampling temperature. Default is 1.0.
            max_tokens (int): Maximum tokens for the response. Default is 150.
            top_p (float): Top p sampling. Default is 1.0.
            finetuned_model_start_sequence_only_for_base_models (str): Start sequence for base models.

        Returns:
            str: Generated completion.

        Raises:
            Exception: If an unknown model is specified or other OpenAI API errors occur.
        """
        
        #logger.debug('eval_system_prompt', self.eval_system_prompt)
        try:
            if "gpt" in finetuned_model:
                response = self.client.chat.completions.create(
                    model=finetuned_model,
                    messages=[
                        {"role": "system", "content":  self.generator_system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    frequency_penalty=0,
                    presence_penalty=0,
                    stop=["END"]
                )
                return response.choices[0].message.content 

            else:
                raise ValueError(f"Unknown model: {finetuned_model}")

        except Exception as e:
            raise Exception(f"Error in get_generated_completion: {e}")
        
    def score_response(self, prompt, ground_truth_completion, generated_completion, llm_evaluator_model_name="gpt-3.5-turbo"):
        """
        Generates a scoring prompt, sends it to the language model, and parses the response.

        Args:
            prompt (str): The original prompt used.
            ground_truth_completion (str): The correct or expected response to the prompt.
            generated_completion (str): The response generated by the evaluated model.
            llm_evaluator_model_name (str): Name of the large language model for scoring. Default is "gpt-3.5-turbo".

        Returns:
            tuple: A tuple containing the score and the reasoning behind the score.

        Raises:
            json.JSONDecodeError: If the response from the model is not in valid JSON format.
            Exception: For other exceptions that may occur during API calls or processing.
        """
        system_prompt = "You are an intelligent assistant capable of evaluating responses based on accuracy and relevance."

        input_prompt = (
            f"The correct response to the prompt '{prompt}' was: '{ground_truth_completion}'. "
            f"The response generated by the test model was: '{generated_completion}'. "

        )

        try:
            response = self.client.chat.completions.create(
                model=llm_evaluator_model_name,
                messages=[
                    {"role": "system", "content": self.eval_system_prompt},
                    {"role": "user", "content": input_prompt},
                    {"role": "assistant", "content": ""}
                ],
            )
            response_content = json.loads(response.choices[0].message.content)
            score = response_content["score"]
            score_reason = response_content["score_reason"]

            return score, score_reason

        except json.JSONDecodeError:
            raise json.JSONDecodeError("Failed to parse model response as JSON.")
        except Exception as e:
            raise Exception(f"An unexpected error occurred: {e}")
        
    def evaluate_model(self, finetuned_model, dataset_csv_path, results_csv_path, 
                       temperature=1.0, max_tokens=150, top_p=1.0, 
                       frequency_penalty=0, presence_penalty=0,
                       model_start_sequence="", 
                       llm_evaluator_model_name="gpt-3.5-turbo", dataset_size=100,
                       finetuned_model_start_sequence_only_for_base_models="",
                       experiment_id=1,
                       save_immediately=False):
        """
        Evaluates a given model's performance and generates statistical insights.

        Args:
            finetuned_model (str): Name of the model to evaluate.
            dataset_csv_path (str): Path to the CSV file containing prompts and completions.
            results_csv_path (str): Path to save the evaluated results.
            temperature (float): Sampling temperature for generation. Default is 1.0.
            max_tokens (int): Maximum tokens for generated responses. Default is 150.
            top_p (float): Top p sampling for generation. Default is 1.0.
            frequency_penalty (float): Frequency penalty parameter. Default is 0.
            presence_penalty (float): Presence penalty parameter. Default is 0.
            model_start_sequence (str): Start sequence for the model, if applicable.
            llm_evaluator_model_name (str): Name of the large language model for evaluation. Default is "gpt-3.5-turbo".
            dataset_size (int): Size of the dataset for evaluation. Default is 100.
            finetuned_model_start_sequence_only_for_base_models (str): Start sequence for base models.
            experiment_id (int): Identifier for the experiment. Default is 1.
            save_immediately (bool): If True, saves each evaluation result immediately. Default is False.

        Returns:
            None

        Raises:
            FileNotFoundError: If the input CSV file is not found.
            ValueError: If the input file is not a CSV or missing required columns.
            Exception: For other exceptions that may occur during processing.
        """
        if not os.path.exists(dataset_csv_path):
            raise FileNotFoundError(f"[evaluate_model] The input CSV file {dataset_csv_path} was not found.")

        # Check if the file extension is .csv
        if not dataset_csv_path.endswith('.csv'):
          raise ValueError(f"[evaluate_model] The input file {dataset_csv_path} is not a CSV file. Make sure that input_csv of the dataset to evaluate is in a CSV Format.")

        

        try:
            df_input = pd.read_csv(dataset_csv_path)

            # Required columns
            required_columns = ['initial_prompt', 'prompt', 'ground_truth_completion', 'classification']

            # Check if all required columns are present
            if not all(column in df_input.columns for column in required_columns):
                missing_columns = [column for column in required_columns if column not in df_input.columns]
                raise ValueError(f"The CSV file is missing the following required columns: {', '.join(missing_columns)}")

            df_input = pd.read_csv(dataset_csv_path)
            all_frames = []

            for index, row in df_input.iterrows():
                prompt = row["prompt"]
                ground_truth_completion = row["ground_truth_completion"]
                logger.debug(prompt)
                generated_completion = self.get_generated_completion(finetuned_model, prompt, temperature, max_tokens, top_p, 
                                                                     finetuned_model_start_sequence_only_for_base_models)
                #logger.debug("*******> generated_completion: ", generated_completion)
                # Assuming the existence of a scoring function here
                score, scoring_reason = self.score_response(prompt, ground_truth_completion, generated_completion)

                # Get the current timestamp
                current_time = datetime.now()
                timestamp_str = current_time.strftime("%Y%m%d%H%M%S")

                # Add the timestamp to the output_data dictionary
                output_data = {
                    "Timestamp": timestamp_str,
                    "Prompt": prompt,
                    "Ground Truth": row["ground_truth_completion"],
                    "Generated Response": generated_completion,
                    "Temperature": temperature,
                    "Max Tokens": max_tokens,
                    "Top P": top_p,
                    "Frequency Penalty": frequency_penalty,
                    "Presence Penalty": presence_penalty,
                    "Score": score,
                    "Classification": row["classification"],
                    "Model Name": finetuned_model,
                    "Dataset Size": dataset_size,
                    "GPT Eval Model": llm_evaluator_model_name,
                    "Scoring Reason": scoring_reason,
                    "Experiment_ID": experiment_id
                }

                # Append the dictionary to the list
                all_frames.append(output_data)

                # Print for debugging
                
                # Corrected logger.debug statement:
                logger.debug("row[%s] Score: %s Prompt: %s %s", index, score, prompt[:50], finetuned_model)

                # Similarly, update other logger.debug statements
                logger.debug("ground_truth_completion: %s", ground_truth_completion[:100])
                logger.debug("generated_completion: %s", generated_completion[:100])
                logger.debug("scoring: %s", score)
                logger.debug("scoring_reason: %s", scoring_reason)
                # Append the dictionary to the list

                # Save the data immediately if specified
                if save_immediately:
                    # Check if the output CSV file already exists
                    file_exists = os.path.exists(results_csv_path)
                    # Append the new data to the CSV file
                    mode = 'a' if file_exists else 'w'
                    header = not file_exists  # Write header only if file doesn't exist
                    pd.DataFrame([output_data]).to_csv(results_csv_path, mode=mode, header=header, index=False)                

                # To avoid hitting rate limits
                time.sleep(1.0)  

            if not save_immediately:
                # Create a DataFrame from the list of dictionaries
                df = pd.DataFrame(all_frames)
                # Save the DataFrame to a CSV file
                df.to_csv(results_csv_path, index=False)

            # Generate statistics and save them
            #self.save_stats(final_df, psugpt_model)

        except pd.errors.ParserError as e:
          raise ValueError(f"The file {dataset_csv_path} could not be parsed as CSV: {e}")

        except openai.APIConnectionError as e:                            
                logger.error(f"[evaluate_model] APIConnectionError error:\n{e}")

        except openai.RateLimitError as e:
            # If the request fails due to rate error limit, increment the retry counter, sleep for 0.5 seconds, and then try again
            logger.error(f"[evaluate_model] RateLimit Error {e}. Trying again in 0.5 seconds...")

        except openai.APIStatusError as e:
            logger.error(f"[evaluate_model] APIStatusError:\n{e}")
            # If the request fails due to service unavailability, sleep for 10 seconds and then try again without incrementing the retry counter

        except AttributeError as e:            
            logger.error(f"[evaluate_model] AttributeError:\n{e}")
            # You can also add additional error handling code here if needed

        except Exception as e:
            raise Exception(f"An error occurred during model evaluation: {e}")